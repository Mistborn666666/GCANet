{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b68c4a",
   "metadata": {},
   "source": [
    "# Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4436138b",
   "metadata": {},
   "source": [
    "## Requirment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72fe33b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:47.291421Z",
     "start_time": "2025-11-25T09:10:45.164786Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset, ConcatDataset\n",
    "import torchaudio\n",
    "from torchaudio import transforms\n",
    "import math, copy, time, random, os\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import scipy.sparse as sp\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from einops import rearrange\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import re\n",
    "from scipy.io import loadmat\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f96dda6",
   "metadata": {},
   "source": [
    "## Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc6c6851",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:47.323268Z",
     "start_time": "2025-11-25T09:10:47.294170Z"
    }
   },
   "outputs": [],
   "source": [
    "class Config():\n",
    "    root = '/home/test/Desktop/python/EEG_data/AAD_dataset/AAD_DTU/Processed/Dataset'\n",
    "    root_feature = '/home/test/Desktop/python/2023/dr/Auditory_Attention_Detection/Features/DTU_1s'\n",
    "    file_name = 'S1_Dataset_1s.npz'\n",
    "    feature_name = 'S1_Features.npz'\n",
    "\n",
    "    current_fold = 5\n",
    "    tim_len = 1\n",
    "    chan_num = 64\n",
    "    band_num = 5\n",
    "    inc = 32\n",
    "    eeg_len = 128\n",
    "    audio_len = 16000\n",
    "    mode = 'cross_trails'\n",
    "    seed = 3407\n",
    "    batch_size = 8\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    weight_decay = 1e-4\n",
    "    lr = 1e-4\n",
    "    patience = 2\n",
    "    factor = 0.8\n",
    "    epochs = 30\n",
    "    dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "542bf464",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a7787d2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:48.295939Z",
     "start_time": "2025-11-25T09:10:47.324728Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_size(net, keyword=None):\n",
    "    if net is not None and isinstance(net, torch.nn.Module):\n",
    "        module_parameters = filter(lambda p: p.requires_grad, net.parameters())\n",
    "        params = sum([np.prod(p.size()) for p in module_parameters])\n",
    "        \n",
    "        print(\"{} Parameters: {:.6f}M\".format(\n",
    "            net.__class__.__name__, params / 1e6), flush=True, end=\"; \")\n",
    "        \n",
    "        if keyword is not None:\n",
    "            keyword_parameters = [p for name, p in net.named_parameters() if p.requires_grad and keyword in name]\n",
    "            params = sum([np.prod(p.size()) for p in keyword_parameters])\n",
    "            print(\"{} Parameters: {:.6f}M\".format(\n",
    "                keyword, params / 1e6), flush=True, end=\"; \")\n",
    "        \n",
    "        print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fd6c585",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:48.387642Z",
     "start_time": "2025-11-25T09:10:48.298628Z"
    }
   },
   "outputs": [],
   "source": [
    "class AvgMeter:\n",
    "    def __init__(self, name=\"Metric\"):\n",
    "        self.name = name\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg, self.sum, self.count = [0] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += val * count\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __repr__(self):\n",
    "        text = f\"{self.name}: {self.avg:.4f}\"\n",
    "        return text\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group[\"lr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1825dbc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:48.479717Z",
     "start_time": "2025-11-25T09:10:48.389543Z"
    }
   },
   "outputs": [],
   "source": [
    "def pre_evaluate(dataloader, model):\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for eeg, eeg_feature, wavA, wavB, event in dataloader:\n",
    "            eeg = eeg.to(Config.device)\n",
    "            eeg_feature = eeg_feature.to(Config.device)\n",
    "            wavA = wavA.to(Config.device)\n",
    "            wavB = wavB.to(Config.device)\n",
    "            event = event.to(Config.device)\n",
    "            event = event.squeeze()\n",
    "            pred = model(eeg, eeg_feature, wavA, wavB)\n",
    "            _, predicted = torch.max(pred, 1)\n",
    "            correct += (predicted == event).sum().item()\n",
    "    accuracy = correct / len(dataloader.dataset)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff01e6d",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afdcf95",
   "metadata": {},
   "source": [
    "## Temporal block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5764b19",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:48.572459Z",
     "start_time": "2025-11-25T09:10:48.481601Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "class down_sample(nn.Module):\n",
    "    def __init__(self, inc, kernel_size, stride, padding):\n",
    "        super(down_sample, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels = inc, out_channels = inc, kernel_size = (1, kernel_size), stride = (1, stride), padding = (0, padding), bias = False)\n",
    "        self.bn = nn.BatchNorm2d(inc) \n",
    "        self.elu = nn.ELU(inplace = False)\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.elu(self.bn(self.conv(x)))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ca47ae3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:48.665145Z",
     "start_time": "2025-11-25T09:10:48.574109Z"
    }
   },
   "outputs": [],
   "source": [
    "class Residual_Block(nn.Module): \n",
    "    def __init__(self, inc, outc, groups = 1):\n",
    "        super(Residual_Block, self).__init__()\n",
    "        if inc is not outc:\n",
    "            self.conv_expand = nn.Conv2d(in_channels = inc, out_channels = outc, kernel_size = 1, \n",
    "                                       stride = 1, padding = 0, groups = groups, bias = False)\n",
    "        else:\n",
    "            self.conv_expand = None\n",
    "          \n",
    "        self.conv1 = nn.Conv2d(in_channels = inc, out_channels = outc, kernel_size = (1, 3), \n",
    "                               stride = 1, padding = (0, 1), groups = groups, bias = False)\n",
    "        self.bn1 = nn.BatchNorm2d(outc)\n",
    "        self.conv2 = nn.Conv2d(in_channels = outc, out_channels = outc, kernel_size = (1, 3), \n",
    "                               stride = 1, padding = (0, 1), groups = groups, bias = False)\n",
    "        self.bn2 = nn.BatchNorm2d(outc)\n",
    "        self.elu = nn.ELU(inplace = False)\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(m.weight, gain = 1)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        \n",
    "    def forward(self, x): \n",
    "        if self.conv_expand is not None:\n",
    "            identity_data = self.conv_expand(x)\n",
    "        else:\n",
    "            identity_data = x\n",
    "        output = self.bn1(self.conv1(x))\n",
    "        output = self.bn2(self.conv2(output))\n",
    "        output = torch.add(output,identity_data)\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4acec5aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:48.756374Z",
     "start_time": "2025-11-25T09:10:48.666986Z"
    }
   },
   "outputs": [],
   "source": [
    "class input_layer(nn.Module):\n",
    "    def __init__(self, outc, groups):\n",
    "        super(input_layer, self).__init__()\n",
    "        self.conv_input = nn.Conv2d(in_channels = 1, out_channels = outc, kernel_size = (1, 3), \n",
    "                                    stride = 1, padding = (0, 1), groups = groups, bias = False)\n",
    "        self.bn_input = nn.BatchNorm2d(outc) \n",
    "        self.elu = nn.ELU(inplace = False)\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(m.weight, gain = 1)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.bn_input(self.conv_input(x))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46b71f96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:48.870308Z",
     "start_time": "2025-11-25T09:10:48.758037Z"
    }
   },
   "outputs": [],
   "source": [
    "def embedding_network(input_block, Residual_Block, num_of_layer, outc, groups = 1):\n",
    "    layers = []\n",
    "    layers.append(input_block(outc,groups=groups))\n",
    "    for i in range(0, num_of_layer):\n",
    "        layers.append(Residual_Block(inc = int(math.pow(2, i)*outc), outc = int(math.pow(2, i+1)*outc),\n",
    "                                     groups = groups))\n",
    "    return nn.Sequential(*layers) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b147bcb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:48.963909Z",
     "start_time": "2025-11-25T09:10:48.874101Z"
    }
   },
   "outputs": [],
   "source": [
    "class Multi_Scale_Temporal_Block(nn.Module):\n",
    "    def __init__(self, outc, num_of_layer = 1):\n",
    "        super().__init__() \n",
    "        self.num_of_layer = num_of_layer\n",
    "        self.embedding = embedding_network(input_layer, Residual_Block, num_of_layer = num_of_layer, outc = outc, groups=1)    \n",
    "\n",
    "        self.downsampled1 = down_sample(outc*int(math.pow(2, num_of_layer))+1, 4, 4, 0)\n",
    "        self.downsampled2 = down_sample(outc*int(math.pow(2, num_of_layer))+1, 8, 8, 0)\n",
    "        self.downsampled3 = down_sample(outc*int(math.pow(2, num_of_layer))+1, 16, 16, 0)\n",
    "        self.downsampled4 = down_sample(outc*int(math.pow(2, num_of_layer))+1, 32, 32, 0)\n",
    "        self.downsampled5 = down_sample(outc*int(math.pow(2, num_of_layer))+1, 32, 32, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        embedding_x = self.embedding(x)\n",
    "        cat_x = torch.cat((embedding_x, x), 1)\n",
    "\n",
    "        downsample1 = self.downsampled1(cat_x)\n",
    "        downsample2 = self.downsampled2(cat_x)\n",
    "        downsample3 = self.downsampled3(cat_x)\n",
    "        downsample4 = self.downsampled4(cat_x)\n",
    "        downsample5 = self.downsampled5(cat_x)\n",
    "\n",
    "        temporal_fe = torch.concat((downsample1,downsample2,downsample3,downsample4,downsample5),3)\n",
    "\n",
    "        return temporal_fe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9da2ed81",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:49.054554Z",
     "start_time": "2025-11-25T09:10:48.965701Z"
    }
   },
   "outputs": [],
   "source": [
    "class Temporal_Block(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.mstblock1 = Multi_Scale_Temporal_Block(outc=2)\n",
    "        self.mstblock2 = Multi_Scale_Temporal_Block(outc=2)\n",
    "        self.mstblock3 = Multi_Scale_Temporal_Block(outc=2)\n",
    "        self.mstblock4 = Multi_Scale_Temporal_Block(outc=2)\n",
    "        self.mstblock5 = Multi_Scale_Temporal_Block(outc=2)\n",
    "\n",
    "        self.fc = nn.Linear(640,256)\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        t_fe1 = self.mstblock1(x[:,0,:,:].unsqueeze(1))\n",
    "        t_fe2 = self.mstblock2(x[:,1,:,:].unsqueeze(1))\n",
    "        t_fe3 = self.mstblock3(x[:,2,:,:].unsqueeze(1))\n",
    "        t_fe4 = self.mstblock4(x[:,3,:,:].unsqueeze(1))\n",
    "        t_fe5 = self.mstblock5(x[:,4,:,:].unsqueeze(1))\n",
    "        t_fe = torch.cat((t_fe1,t_fe2,t_fe3,t_fe4,t_fe5),1)\n",
    "\n",
    "        return t_fe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f3101bf",
   "metadata": {},
   "source": [
    "## GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6945ac2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:49.158497Z",
     "start_time": "2025-11-25T09:10:49.056333Z"
    }
   },
   "outputs": [],
   "source": [
    "class Electrodes:\n",
    "    def __init__(self):\n",
    "        self.positions_3d = np.array([\n",
    "            [-0.029338731, 0.09029533, -0.003315452],\n",
    "            [-0.055805583, 0.076809795, -0.003315452],\n",
    "            [-0.038593441, 0.082763901, 0.026185549],\n",
    "            [-0.027261703, 0.067475084, 0.061064823],\n",
    "            [-0.051775707, 0.063937674, 0.0475],\n",
    "            [-0.06925438, 0.060201914, 0.024587809],\n",
    "            [-0.076809795, 0.055805583, -0.003315452],\n",
    "            [-0.09029533, 0.029338731, -0.003315452],\n",
    "            [-0.084349336, 0.032378676, 0.029356614],\n",
    "            [-0.064255824, 0.034165428, 0.061064823],\n",
    "            [-0.035597403, 0.035597403, 0.080564569],\n",
    "            [-0.037119457, 0, 0.087447961],\n",
    "            [-0.068337281, 0, 0.065992545],\n",
    "            [-0.088690141, 0, 0.034044955],\n",
    "            [-0.094942129, 0, -0.003315452],\n",
    "            [-0.09029533, -0.029338731, -0.003315452],\n",
    "            [-0.084349336, -0.032378676, 0.029356614],\n",
    "            [-0.064255824, -0.034165428, 0.061064823],\n",
    "            [-0.035597403, -0.035597403, 0.080564569],\n",
    "            [-0.027261703, -0.067475084, 0.061064823],\n",
    "            [-0.051775707, -0.063937674, 0.0475],\n",
    "            [-0.06925438, -0.060201914, 0.024587809],\n",
    "            [-0.076809795, -0.055805583, -0.003315452],\n",
    "            [-0.069655748, -0.050607863, -0.040148735],\n",
    "            [-0.055805583, -0.076809795, -0.003315452],\n",
    "            [-0.038593441, -0.082763901, 0.026185549],\n",
    "            [-0.029338731, -0.09029533, -0.003315452],\n",
    "            [5.27206E-18, -0.08609924, -0.040148735],\n",
    "            [5.81353E-18, -0.094942129, -0.003315452],\n",
    "            [5.4307E-18, -0.088690141, 0.034044955],\n",
    "            [4.18445E-18, -0.068337281, 0.065992545],\n",
    "            [2.27291E-18, -0.037119457, 0.087447961],\n",
    "            [5.81353E-18, 0.094942129, -0.003315452],\n",
    "            [0.029338731, 0.09029533, -0.003315452],\n",
    "            [0.055805583, 0.076809795, -0.003315452],\n",
    "            [0.038593441, 0.082763901, 0.026185549],\n",
    "            [5.4307E-18, 0.088690141, 0.034044955],\n",
    "            [4.18445E-18, 0.068337281, 0.065992545],\n",
    "            [0.027261703, 0.067475084, 0.061064823],\n",
    "            [0.051775707, 0.063937674, 0.0475],\n",
    "            [0.06925438, 0.060201914, 0.024587809],\n",
    "            [0.076809795, 0.055805583, -0.003315452],\n",
    "            [0.09029533, 0.029338731, -0.003315452],\n",
    "            [0.084349336, 0.032378676, 0.029356614],\n",
    "            [0.064255824, 0.034165428, 0.061064823],\n",
    "            [0.035597403, 0.035597403, 0.080564569],\n",
    "            [2.27291E-18, 0.037119457, 0.087447961],\n",
    "            [0, 0, 0.095],\n",
    "            [0.037119457, 0, 0.087447961],\n",
    "            [0.068337281, 0, 0.065992545],\n",
    "            [0.088690141, 0, 0.034044955],\n",
    "            [0.094942129, 0, -0.003315452],\n",
    "            [0.09029533, -0.029338731, -0.003315452],\n",
    "            [0.084349336, -0.032378676, 0.029356614],\n",
    "            [0.064255824, -0.034165428, 0.061064823],\n",
    "            [0.035597403, -0.035597403, 0.080564569],\n",
    "            [0.027261703, -0.067475084, 0.061064823],\n",
    "            [0.051775707, -0.063937674, 0.0475],\n",
    "            [0.06925438, -0.060201914, 0.024587809],\n",
    "            [0.076809795, -0.055805583, -0.003315452],\n",
    "            [0.069655748, -0.050607863, -0.040148735],\n",
    "            [0.055805583, -0.076809795, -0.003315452],\n",
    "            [0.038593441, -0.082763901, 0.026185549],\n",
    "            [0.029338731, -0.09029533, -0.003315452]])\n",
    "        self.positions_3d = np.int_(self.positions_3d * 1000)\n",
    "        self.channel_names = np.array([\n",
    "            'Fp1', 'AF7', 'AF3', 'F1', 'F3', 'F5', 'F7', 'FT7', 'FC5', 'FC3', 'FC1', 'C1', 'C3', 'C5', 'T7', 'TP7',\n",
    "            'CP5', 'CP3', 'CP1', 'P1', 'P3', 'P5', 'P7', 'P9', 'PO7', 'PO3', 'O1', 'Iz', 'Oz', 'POz', 'Pz', 'CPz',\n",
    "            'Fpz', 'Fp2', 'AF8', 'AF4', 'AFz', 'Fz', 'F2', 'F4', 'F6', 'F8', 'FT8', 'FC6', 'FC4', 'FC2', 'FCz', 'Cz',\n",
    "            'C2', 'C4', 'C6', 'T8', 'TP8', 'CP6', 'CP4', 'CP2', 'P2', 'P4', 'P6', 'P8', 'P10', 'PO8', 'PO4', 'O2'])\n",
    "        self.channel_to_index = {name: idx for idx, name in enumerate(self.channel_names)}\n",
    "        self.edge_importance = nn.Parameter(\n",
    "            torch.eye(64, device='cuda') * 0.1 + torch.randn(64, 64, device='cuda') * 0.01,\n",
    "            requires_grad=True\n",
    "        )\n",
    "\n",
    "\n",
    "    def get_adjacency_matrix(self, calibration_constant=6, active_threshold=0.1):\n",
    "        distance_3d_matrix = np.linalg.norm(self.positions_3d[:, np.newaxis] - self.positions_3d, axis=-1)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            distance_3d_matrix = np.where(distance_3d_matrix != 0, calibration_constant / distance_3d_matrix, 0)\n",
    "\n",
    "        local_conn_mask = distance_3d_matrix > active_threshold\n",
    "        local_connections = distance_3d_matrix * local_conn_mask\n",
    "\n",
    "        np.fill_diagonal(local_connections, 0)\n",
    "        min_conn = local_connections.min()\n",
    "        max_conn = local_connections.max()\n",
    "        if max_conn > min_conn:\n",
    "            adj_matrix = (local_connections - min_conn) / (max_conn - min_conn)\n",
    "        else:\n",
    "            adj_matrix = np.zeros_like(local_connections)\n",
    "        np.fill_diagonal(adj_matrix, 1)\n",
    "\n",
    "\n",
    "        return adj_matrix\n",
    "    def get_adj(self):\n",
    "        base_adj_matrix = self.get_adjacency_matrix()\n",
    "        A = torch.tensor(base_adj_matrix, dtype=torch.float32, device=self.edge_importance.device) + self.edge_importance\n",
    "        A = (A + A.t()) / 2.0\n",
    "        I = torch.eye(A.size(-1), device=A.device)\n",
    "        A = A + I\n",
    "        rowsum = A.sum(dim=-1, keepdim=True)\n",
    "        d_inv_sqrt = rowsum.pow(-0.5)\n",
    "        d_inv_sqrt[torch.isinf(d_inv_sqrt)] = 0.0 \n",
    "        A = d_inv_sqrt * A * d_inv_sqrt.transpose(-1, -2)\n",
    "\n",
    "        return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cd098627",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:49.274538Z",
     "start_time": "2025-11-25T09:10:49.160570Z"
    }
   },
   "outputs": [],
   "source": [
    "class GATENet(nn.Module):\n",
    "    def __init__(self, inc, reduction_ratio=128):\n",
    "        super(GATENet, self).__init__()\n",
    "        self.fc = nn.Sequential(nn.Linear(inc, inc // reduction_ratio, bias=False),\n",
    "                                nn.ELU(inplace=False),\n",
    "                                nn.Linear(inc // reduction_ratio, inc, bias=False),\n",
    "                                nn.Tanh(),\n",
    "                                nn.ReLU(inplace=False))\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.fc(x)\n",
    "        return y\n",
    "\n",
    "\n",
    "class resGCN(nn.Module):\n",
    "    def __init__(self, inc, outc,band_num):\n",
    "        super(resGCN, self).__init__()\n",
    "        self.GConv1 = nn.Conv2d(in_channels=inc,\n",
    "                                out_channels=outc,\n",
    "                                kernel_size=(1, 3),\n",
    "                                stride=(1, 1),\n",
    "                                padding=(0, 0),\n",
    "                                groups=band_num,\n",
    "                                bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(outc)\n",
    "        self.GConv2 = nn.Conv2d(in_channels=outc,\n",
    "                                out_channels=outc,\n",
    "                                kernel_size=(1, 1),\n",
    "                                stride=(1, 1),\n",
    "                                padding=(0, 1),\n",
    "                                groups=band_num,\n",
    "                                bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(outc)\n",
    "        self.ELU = nn.ELU(inplace=False)\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, x_p, L):\n",
    "        x = self.bn2(self.GConv2(self.ELU(self.bn1(self.GConv1(x)))))\n",
    "        y = torch.einsum('bijk,kp->bijp', (x, L))\n",
    "        y = self.ELU(torch.add(y, x_p))\n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "class HGCN(nn.Module):\n",
    "    def __init__(self, dim, chan_num, band_num):\n",
    "        super(HGCN, self).__init__()\n",
    "        self.chan_num = chan_num\n",
    "        self.dim = dim\n",
    "        self.electrodes = Electrodes() \n",
    "        self.resGCN = resGCN(inc=dim * band_num, outc=dim * band_num, band_num=band_num)\n",
    "        self.ELU = nn.ELU(inplace=False)\n",
    "        self.initialize()\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_uniform_(m.weight, gain=1)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    def forward(self, x, A_ds):  \n",
    "        A_ds_tensor = torch.tensor(A_ds, dtype=torch.float32).to(x.device)  \n",
    "        edge_importance = self.electrodes.edge_importance\n",
    "        A = A_ds_tensor + edge_importance\n",
    "        L = torch.einsum('ik,kp->ip', \n",
    "                         (A, torch.diag(torch.reciprocal(torch.sum(A, dim=-1)))))\n",
    "        x = x.permute(0, 1, 3, 2)\n",
    "        G = self.resGCN(x, x, L).contiguous()\n",
    "        return G.squeeze(2).transpose(2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3e5bed26",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:49.429910Z",
     "start_time": "2025-11-25T09:10:49.276013Z"
    }
   },
   "outputs": [],
   "source": [
    "class SGCN(nn.Module):\n",
    "    def __init__(self, dim, device=Config.device):\n",
    "        super().__init__()\n",
    "        self.electrodes = Electrodes() \n",
    "        self.GATENet = GATENet(Config.chan_num * Config.chan_num, reduction_ratio=Config.chan_num)\n",
    "        self.gcn = HGCN(dim=dim, chan_num=Config.chan_num, band_num=Config.band_num)\n",
    "\n",
    "    def forward(self, x):\n",
    "        A_ds = self.electrodes.get_adjacency_matrix()  \n",
    "        feat = self.gcn(x, A_ds)\n",
    "\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a60fa0f",
   "metadata": {},
   "source": [
    "## EEGEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c713ae2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:49.842128Z",
     "start_time": "2025-11-25T09:10:49.431748Z"
    }
   },
   "outputs": [],
   "source": [
    "class EEGEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__() \n",
    "        self.t_block = Temporal_Block()\n",
    "        self.tgcn = SGCN(dim=5)\n",
    "        self.dgcn = SGCN(dim=1)\n",
    "        self.pgcn = SGCN(dim=1)\n",
    "\n",
    "        self.chanattn = nn.Conv2d(in_channels=int(Config.chan_num*Config.tim_len),out_channels=Config.inc,kernel_size=1)\n",
    "\n",
    "        self.proj1 = nn.Linear(Config.inc*25,int(Config.chan_num*Config.tim_len))\n",
    "        self.proj2 = nn.Linear(5,int(Config.chan_num*Config.tim_len*0.5))\n",
    "        self.proj3 = nn.Linear(5,int(Config.chan_num*Config.tim_len*0.5))\n",
    "        self.dpout = nn.Dropout(p=0.5)\n",
    "\n",
    "    def forward(self, filtered, prefeat):\n",
    "        filtered = filtered.unsqueeze(1).expand(-1, 5, -1, -1)\n",
    "        temporal_fe = self.t_block(filtered)\n",
    "        t_fe = self.tgcn(temporal_fe)\n",
    "        t_fe = self.chanattn(t_fe).permute(0,3,2,1).contiguous()\n",
    "        t_fe = self.proj1(t_fe.view(prefeat.shape[0],prefeat.shape[1],-1))\n",
    "        d_fe = self.dgcn(prefeat[:,:,:5].permute(0,2,1).unsqueeze(3))\n",
    "        p_fe = self.pgcn(prefeat[:,:,5:].permute(0,2,1).unsqueeze(3))\n",
    "        p_fe = self.proj2(p_fe)\n",
    "        d_fe = self.proj3(d_fe)\n",
    "        fe = torch.cat((t_fe,d_fe,p_fe),dim=2)\n",
    "        return fe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac00800f",
   "metadata": {},
   "source": [
    "## AudioEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04e0f723",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:49.959351Z",
     "start_time": "2025-11-25T09:10:49.844020Z"
    }
   },
   "outputs": [],
   "source": [
    "class AudioEncoder(nn.Module):\n",
    "    def __init__(self, in_channels=1, out_channels=Config.chan_num, input_length=int(Config.audio_len*Config.tim_len), target_length=64):\n",
    "        super(AudioEncoder, self).__init__()\n",
    "        self.downsample_factor = input_length // target_length  \n",
    "        self.conv1 = nn.Conv1d(in_channels, 16, kernel_size=3, stride=5, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(16)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, stride=5, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(32)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv3 = nn.Conv1d(32, out_channels, kernel_size=3, stride=5, padding=1)\n",
    "        self.bn3 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu3 = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        \n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        \n",
    "        return x "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9deb30",
   "metadata": {},
   "source": [
    "## Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3dca2e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:50.079153Z",
     "start_time": "2025-11-25T09:10:49.961193Z"
    }
   },
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, temperature, attn_dropout=0.1):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.temperature = temperature\n",
    "        self.dropout = nn.Dropout(attn_dropout)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        attn = torch.matmul(q / self.temperature, k.transpose(2, 3)) \n",
    "        if mask is not None:\n",
    "            attn = attn.masked_fill(mask == 0, -1e9)\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.dropout(attn)\n",
    "        output = torch.matmul(attn, v) \n",
    "        \n",
    "        return output, attn\n",
    "\n",
    "class ConvCrossAttention(nn.Module):\n",
    "    def __init__(self, n_head, d_model, d_k, d_v, in_ch,\n",
    "                 kernel_size, dilation, dropout=0.1):\n",
    "        super(ConvCrossAttention, self).__init__()\n",
    "        self.n_head = n_head\n",
    "        self.in_ch = in_ch\n",
    "        head_dim = in_ch // n_head\n",
    "        self.d_k = head_dim\n",
    "        self.d_v = head_dim\n",
    "\n",
    "        self.q_proj = nn.Linear(in_ch, n_head * head_dim)\n",
    "        self.k_proj = nn.Linear(in_ch, n_head * head_dim)\n",
    "        self.v_proj = nn.Linear(in_ch, n_head * head_dim)\n",
    "\n",
    "        self.out_proj = nn.Linear(n_head * head_dim, in_ch)\n",
    "\n",
    "        self.attention = ScaledDotProductAttention(temperature=head_dim ** 0.5)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layer_norm = nn.GroupNorm(1, in_ch, eps=1e-8)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        residual = v  \n",
    "        q = q.transpose(1, 2) \n",
    "        k = k.transpose(1, 2) \n",
    "        v = v.transpose(1, 2)  \n",
    "\n",
    "        B, len_q, _ = q.size()\n",
    "        len_k = k.size(1)\n",
    "        len_v = v.size(1)\n",
    "\n",
    "        n_head, d_k, d_v = self.n_head, self.d_k, self.d_v\n",
    "\n",
    "        q = self.q_proj(q).view(B, len_q, n_head, d_k) \n",
    "        k = self.k_proj(k).view(B, len_k, n_head, d_k)  \n",
    "        v = self.v_proj(v).view(B, len_v, n_head, d_v)  \n",
    "\n",
    "        q = q.transpose(1, 2) \n",
    "        k = k.transpose(1, 2)  \n",
    "        v = v.transpose(1, 2) \n",
    "\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)  \n",
    "        out, attn = self.attention(q, k, v, mask=mask)  \n",
    "        out = out.transpose(1, 2).contiguous().view(B, len_q, n_head * d_v) \n",
    "        out = self.out_proj(out)  \n",
    "        out = out.transpose(1, 2) \n",
    "        out = self.dropout(out)\n",
    "        out = out + residual\n",
    "        out = self.layer_norm(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiLayerCrossAttention(nn.Module):\n",
    "    def __init__(self, input_size=int(Config.eeg_len * Config.tim_len),\n",
    "                 layer=4, in_ch=Config.chan_num, kernel_size=3, dilation=1):\n",
    "        super(MultiLayerCrossAttention, self).__init__()\n",
    "        self.layer = layer\n",
    "        self.in_ch = in_ch\n",
    "        self.spike_encoder = nn.ModuleList()\n",
    "        self.LayernormList_spike = nn.ModuleList()\n",
    "        self.projection = nn.Conv1d(in_ch * 2, in_ch, kernel_size, padding='same')\n",
    "        self.layernorm_out = nn.GroupNorm(1, in_ch, eps=1e-8)\n",
    "        for _ in range(layer):\n",
    "            self.LayernormList_spike.append(nn.GroupNorm(1, in_ch, eps=1e-8))\n",
    "        for _ in range(layer):\n",
    "            self.spike_encoder.append(\n",
    "                ConvCrossAttention(\n",
    "                    n_head=1,\n",
    "                    d_model=input_size,  \n",
    "                    d_k=input_size,\n",
    "                    d_v=input_size,\n",
    "                    in_ch=in_ch,\n",
    "                    kernel_size=kernel_size,\n",
    "                    dilation=dilation,\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, spike, audio):\n",
    "        out_spike = spike                          \n",
    "        skip_spike = torch.zeros_like(spike)       \n",
    "        residual_spike = spike\n",
    "\n",
    "        for i in range(self.layer):\n",
    "            out_spike = self.spike_encoder[i](out_spike, audio, audio)\n",
    "            out_spike = out_spike + residual_spike\n",
    "            out_spike = self.LayernormList_spike[i](out_spike)\n",
    "\n",
    "            residual_spike = out_spike\n",
    "            skip_spike = skip_spike + out_spike\n",
    "\n",
    "        out = torch.cat((skip_spike, spike), dim=1)  \n",
    "        out = self.projection(out)                   \n",
    "        out = self.layernorm_out(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904a60b4",
   "metadata": {},
   "source": [
    "## Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3312784",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:50.194850Z",
     "start_time": "2025-11-25T09:10:50.080855Z"
    }
   },
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.input_stack = nn.Sequential(\n",
    "            nn.Linear(int(Config.eeg_len*Config.tim_len*2), 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 64)\n",
    "        )\n",
    "        self.output_stack = nn.Sequential(\n",
    "            nn.Linear(4096, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x_forward = self.input_stack(x)\n",
    "        x_forward = x_forward.view(x_forward.shape[0], -1)\n",
    "        output = self.output_stack(x_forward)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6cc3da",
   "metadata": {},
   "source": [
    "## GCANet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d4ecffea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:50.312408Z",
     "start_time": "2025-11-25T09:10:50.196478Z"
    }
   },
   "outputs": [],
   "source": [
    "class GCANet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GCANet, self).__init__()\n",
    "        self.eeg_encoder = EEGEncoder()\n",
    "        self.audio_encoder = AudioEncoder()\n",
    "        self.fusion = MultiLayerCrossAttention()\n",
    "        self.classifier = Classifier()\n",
    "        \n",
    "\n",
    "    def forward(self, eeg, eeg_feature, waveA, waveB):\n",
    "        eeg_feature = self.eeg_encoder(eeg, eeg_feature)\n",
    "        waveA_feature = self.audio_encoder(waveA)\n",
    "        waveB_feature = self.audio_encoder(waveB)\n",
    "        aligned_waveA_feature = self.fusion(eeg_feature, waveA_feature)\n",
    "        aligned_waveB_feature = self.fusion(eeg_feature, waveB_feature)\n",
    "        combined_feature = torch.cat((aligned_waveA_feature, aligned_waveB_feature), dim=-1)\n",
    "\n",
    "        classifier = self.classifier(combined_feature)\n",
    "        return classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59d1dcc",
   "metadata": {},
   "source": [
    "# Dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02a8916",
   "metadata": {},
   "source": [
    "## Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cb4a8b04",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:50.429953Z",
     "start_time": "2025-11-25T09:10:50.314419Z"
    }
   },
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)  \n",
    "    torch.cuda.manual_seed_all(seed)  \n",
    "    random.seed(seed)  \n",
    "    np.random.seed(seed)  \n",
    "    torch.backends.cudnn.deterministic = False \n",
    "    torch.backends.cudnn.benchmark = False  \n",
    "    \n",
    "def collate_fn(item):\n",
    "    eeg, eeg_feature, wavA, wavB, event = zip(*item)\n",
    "    return torch.stack(eeg), torch.stack(eeg_feature), torch.stack(wavA), torch.stack(wavB), torch.stack(event)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b8e3d0a",
   "metadata": {},
   "source": [
    "## Cross trails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d18b80f8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:50.552564Z",
     "start_time": "2025-11-25T09:10:50.432119Z"
    }
   },
   "outputs": [],
   "source": [
    "def Dataset_cross_trails(root, file_name, root_feature, feature_name, batch_size, fold_num=None):\n",
    "    set_seed(seed=Config.seed)\n",
    "    TotalDataset = EEGDataset(root, file_name, root_feature, feature_name)\n",
    "    total_len = len(TotalDataset)\n",
    "    kf = KFold(n_splits=5, shuffle=False)\n",
    "    fold_dataloaders = []\n",
    "\n",
    "    for fold_idx, (train_indices, val_indices) in enumerate(kf.split(range(total_len))):\n",
    "        if fold_num is not None and (fold_idx != (fold_num-1)): \n",
    "            continue\n",
    "\n",
    "        Train_dataset = Subset(TotalDataset, train_indices)\n",
    "        Valid_dataset = Subset(TotalDataset, val_indices)\n",
    "        \n",
    "        kwargs = {\"batch_size\": batch_size, \"num_workers\": 4, \"pin_memory\": False, \"drop_last\": False}\n",
    "        \n",
    "        Train_dataloader = DataLoader(Train_dataset, collate_fn=collate_fn, shuffle=True, **kwargs)\n",
    "        Valid_dataloader = DataLoader(Valid_dataset, collate_fn=collate_fn, shuffle=False, **kwargs)\n",
    "        \n",
    "        if fold_num is not None:\n",
    "            return (Train_dataloader, Valid_dataloader) \n",
    "        else:\n",
    "            fold_dataloaders.append((Train_dataloader, Valid_dataloader))\n",
    "    \n",
    "    return fold_dataloaders if fold_num is None else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbb3737",
   "metadata": {},
   "source": [
    "# Pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96304c2b",
   "metadata": {},
   "source": [
    "## Train epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7bbbbc97",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:50.679388Z",
     "start_time": "2025-11-25T09:10:50.554059Z"
    }
   },
   "outputs": [],
   "source": [
    "def preTrain_epoch(model, train_loader, optimizer, lr_scheduler, step):\n",
    "    loss_meter = AvgMeter()\n",
    "    tqdm_object = tqdm(train_loader, total=len(train_loader))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for eeg, eeg_feature, wavA, wavB, event in tqdm_object:\n",
    "        eeg = eeg.to(Config.device)\n",
    "        eeg_feature = eeg_feature.to(Config.device)\n",
    "        wavA = wavA.to(Config.device)\n",
    "        wavB = wavB.to(Config.device)\n",
    "        event = event.to(Config.device).squeeze(-1)\n",
    "        output = model(eeg, eeg_feature, wavA, wavB)\n",
    "        loss= criterion(output, event)\n",
    "\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        count = eeg.size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n",
    "    return loss_meter\n",
    "\n",
    "def preVal_epoch(model, valid_loader):\n",
    "    loss_meter = AvgMeter()\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n",
    "    for eeg, eeg_feature, wavA, wavB, event in tqdm_object:\n",
    "        eeg = eeg.to(Config.device)\n",
    "        eeg_feature = eeg_feature.to(Config.device)\n",
    "        wavA = wavA.to(Config.device)\n",
    "        wavB = wavB.to(Config.device)\n",
    "        event = event.to(Config.device).squeeze(-1)\n",
    "        output = model(eeg, eeg_feature, wavA, wavB)\n",
    "        loss= criterion(output, event)\n",
    "\n",
    "        count = eeg.size(0)\n",
    "        loss_meter.update(loss.item(), count)\n",
    "\n",
    "        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n",
    "    return loss_meter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5ac252",
   "metadata": {},
   "source": [
    "## train_and_save_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cd1896ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:50.810796Z",
     "start_time": "2025-11-25T09:10:50.681304Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_and_save_model(config):\n",
    "    \n",
    "    train_dataloader, valid_dataloader = Dataset_cross_trails(config.root, config.file_name, config.root_feature, config.feature_name, \n",
    "                                                                  config.batch_size, config.current_fold)\n",
    "        \n",
    "    save_dir = f\"/DTU_{Config.mode}_{Config.tim_len}s/fold_{config.current_fold}\"\n",
    "\n",
    "    sub_dir_name = config.file_name.split('_')[0]\n",
    "    final_save_dir = os.path.join(save_dir, sub_dir_name)\n",
    "    if not os.path.exists(final_save_dir):\n",
    "        os.makedirs(final_save_dir)\n",
    "\n",
    "    model = GCANet().to(config.device)\n",
    "\n",
    "    params = [{\"params\": model.parameters(),\"lr\": config.lr, \"weight_decay\": config.weight_decay}]\n",
    "    optimizer = torch.optim.AdamW(params, weight_decay=0.)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode=\"min\", patience=config.patience, factor=config.factor\n",
    "    )\n",
    "    step = \"epoch\"\n",
    "    best_loss = float('inf')\n",
    "    acc_max = 0\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        print(f\"Epoch: {epoch + 1}\")\n",
    "        model.train()\n",
    "        train_loss = preTrain_epoch(model, train_dataloader, optimizer, lr_scheduler, step)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            train_acc = pre_evaluate(train_dataloader, model)\n",
    "            print(f\"train accuracy: {train_acc:.6f}\")\n",
    "            valid_loss = preVal_epoch(model, valid_dataloader)\n",
    "            val_acc = pre_evaluate(valid_dataloader, model)\n",
    "            print(f\"valid accuracy: {val_acc:.6f}\")\n",
    "            if val_acc > acc_max:\n",
    "                acc_max = val_acc\n",
    "                model_filename = f\"{epoch + 1}_best.pt\"\n",
    "                save_path = os.path.join(final_save_dir, model_filename)\n",
    "                torch.save(model.state_dict(), save_path)\n",
    "                print(f\"Saved Best Model as {save_path}!\")\n",
    "\n",
    "        lr_scheduler.step(valid_loss.avg)\n",
    "    print_size(model)\n",
    "    print(f\"Final accuracy max for {config.file_name}: {acc_max:.3f}\")\n",
    "\n",
    "    best_model_files = [f for f in os.listdir(final_save_dir) if f.endswith('_best.pt')]\n",
    "    best_epoch = max(int(f.split('_')[0]) for f in best_model_files)\n",
    "    best_model_filename = f\"{best_epoch}_best.pt\"\n",
    "    best_model_path = os.path.join(final_save_dir, best_model_filename)\n",
    "    new_filename = f\"{Config.mode}_{sub_dir_name.lower()}_best.pt\"\n",
    "    new_file_path = os.path.join(final_save_dir, new_filename)\n",
    "    shutil.copy(best_model_path, new_file_path)\n",
    "    print(f\"Copied and renamed {best_model_filename} to {new_filename}\")\n",
    "    folder_name = f\"{config.file_name.split('.')[0]}_fold{config.current_fold}\"\n",
    "\n",
    "    return folder_name, acc_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ead7be01",
   "metadata": {},
   "source": [
    "## run all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9aa5fd8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-25T09:10:50.949307Z",
     "start_time": "2025-11-25T09:10:50.812771Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_all_files_in_directory(config):\n",
    "    files = [f for f in os.listdir(config.root) if os.path.isfile(os.path.join(config.root, f))]\n",
    "    files.sort(key=lambda x: int(x.split('_')[0][1:]))\n",
    "    feature_files = [f for f in os.listdir(config.root_feature) if os.path.isfile(os.path.join(config.root_feature, f))]\n",
    "    feature_files.sort(key=lambda x: int(x.split('_')[0][1:]))\n",
    "    \n",
    "    if len(files) != len(feature_files):\n",
    "        raise ValueError(\"The number of EEG files and feature files does not match!\")\n",
    "        \n",
    "    results = {}\n",
    "\n",
    "    for eeg_file, feature_file in zip(files, feature_files):\n",
    "        if Config.mode == 'cross_subject':\n",
    "            match = re.search(r's(\\d+)', eeg_file)\n",
    "            subject_number = int(match.group(1))\n",
    "            config.LOO = subject_number\n",
    "\n",
    "        config.file_name = eeg_file  \n",
    "        config.feature_name = feature_file \n",
    "        print(f\"Processing EEG file: {config.file_name}, Feature file: {config.feature_name}\")\n",
    "        subject_acc = []\n",
    "        \n",
    "        for fold_num in range(1, 6):\n",
    "            print(f\"Fold {fold_num}/5\")\n",
    "            config.current_fold = fold_num \n",
    "\n",
    "            folder_name, fold_acc = train_and_save_model(config)\n",
    "            subject_acc.append(fold_acc)\n",
    "            \n",
    "            print(f\"Fold {fold_num} Accuracy: {fold_acc:.3f}\")\n",
    "        \n",
    "        avg_acc = np.mean(subject_acc)\n",
    "        std_acc = np.std(subject_acc)\n",
    "        \n",
    "        sub_dir_name = config.file_name.split('_')[0]\n",
    "        \n",
    "        subj_id = f\"{sub_dir_name}\"\n",
    "        results[subj_id] = {\n",
    "            'avg_acc': avg_acc,\n",
    "            'std_acc': std_acc,\n",
    "            'fold_accs': subject_acc\n",
    "        }\n",
    "        \n",
    "        print(f\"Subject {subj_id} Final: {avg_acc:.3f} ± {std_acc:.3f}\")\n",
    "    \n",
    "    print(\"\\nFinal Results:\")\n",
    "    for subj, metrics in results.items():\n",
    "        print(f\"{subj}:\")\n",
    "        print(f\"  Fold Accuracies: {[f'{x:.3f}' for x in metrics['fold_accs']]}\")\n",
    "        print(f\"  Average Accuracy: {metrics['avg_acc']:.3f}\")\n",
    "        print(f\"  Standard Deviation: {metrics['std_acc']:.3f}\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803ddc24",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = run_all_files_in_directory(Config)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "无",
  "kernelspec": {
   "display_name": "torch_new",
   "language": "python",
   "name": "torch_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "231.25px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
